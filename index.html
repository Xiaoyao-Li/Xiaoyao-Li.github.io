<!--
 * @Author: Puhao Li liph19@mails.tsinghua.edu.cn
 * @Date: 2022-10-03 02:05:44
 * @LastEditors: Puhao Li liph19@mails.tsinghua.edu.cn
 * @LastEditTime: 2023-04-26 02:07:21
 * @FilePath: \Xiaoyao-Li.github.io\index.html
 * @Description: This is the Web Page of Puhao Li
-->
<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Puhao Li | ÊùéÊµ¶Ë±™</title>
  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-DQ29Q4W6T6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-DQ29Q4W6T6');
  </script>

  <meta name="author" content="Puhao Li">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="msvalidate.01" content="1D6EAEB9C6558C0BB977413398D67E91" />
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>ü¶æ</text></svg>">
</head>

<body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:3.2%;width:60%;vertical-align:middle">
                <p style="text-align:center">
                  <name>Puhao Li &nbsp | &nbsp ÊùéÊµ¶Ë±™</name>
                </p>
                <p style="font-size:15px; line-height:1.;">
                  I am currently a Ph.D. student in <a href="https://www.au.tsinghua.edu.cn/" target="_blank">Dept. of Automation, Tsinghua University</a> advised by Prof. <a href="https://www.zhusongchun.net/" target="_blank">Song-Chun Zhu</a>.
                  I am also a research intern in General Vision Lab at <a href="https://bigai.ai/" target="_blank">Beijing Institute for General Artificial Intelligence (BIGAI)</a>, and I am grateful to be advised by Dr. <a href="http://tengyu.ai/" target="_blank">Tengyu Liu</a> and Dr. <a href="https://siyuanhuang.com/" target="_blank">Siyuan Huang</a>. 
                  Previously, I obtained my B.Eng. degree from Tsinghua University in 2023.
                </p>
                <p style="font-size:15px; line-height:1.;">
                    My research interests lie in the intersection of robotics manipulation and 3D computer vision.
                    My long-term goal is to develop embodied intelligent systems capable of interpreting human intent and naturally interacting with people in various environments, learning reusable and endless low-level skill sets and high-level common sense.
                    Currently, I am working on 3D scene understanding and robotic manipulation learning, pushing the boundaries of how robots operate within complex settings.
                </p>
                <p style="text-align:center; font-size:16px; font-weight:initial;">
                  <a href="mailto:puhaoli01@gmail.com">Email</a> &nbsp/&nbsp
                  <a href="https://xiaoyao-li.github.io/files/cv_recent.pdf">CV</a> &nbsp/&nbsp
                  <a href="https://scholar.google.at/citations?user=HTsO18AAAAAJ&hl=en&oi=ao" target="_blank">Google Scholar</a> &nbsp/&nbsp
                  <a href="https://github.com/Xiaoyao-Li/" target="_blank">Github</a> &nbsp/&nbsp
                  <a href="https://twitter.com/lpho01031880" target="_blank">Twitter</a>
                </p>
              </td>
              <td style="padding:5%;width:30%;max-width:30%">
                <a href="images/personal/PuhaoLi.jpg"><img style="width:100%;max-width:100%;object-fit: cover; border-radius: 12%;" alt="profile photo" src="images/personal/PuhaoLi.jpg" class="hoverZoomLink"></a>
              </td>
            </tr>
    </tbody></table>

    <!-- RESEARCH -->
    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr>
          <td style="width:100%;vertical-align:middle;padding-left:0px;padding-top:0px;">
            <heading>Research</heading>
          </td>
        </tr>
    </tbody></table>
    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      
      <!-- example -->
      <!-- <tr onmouseout="ag2manip_stop()" onmouseover="ag2manip_start()">
        <td style="width:30%;vertical-align:middle;padding-left:0px;padding-top:25px">
          <div class="one">
            <div class="two" id='ag2manip_image'>
              <img src='images/research/Ag2Manip.jpg' width="220"></div>
            <img src='images/research/Ag2Manip.jpg' width="220">
          </div>
          <script type="text/javascript">
            function ag2manip_start() {
              document.getElementById('ag2manip_image').style.opacity = "1";
            }

            function ag2manip_stop() {
              document.getElementById('ag2manip_image').style.opacity = "0";
            }
            ag2manip_stop()
          </script>
        </td>
        <td style="width:75%;vertical-align:middle;padding-left:5px;padding-top:0px">
          <papertitle>Title Title Title Title Title Title Title Title Title Title</papertitle>
          <br>
          Puhao Li,
          <a href="#" target="_blank">Co-authors</a>,
          <br>
          <em>arXiv</em> 2024
          <br>
          <a href="#" target="_blank">[Paper]</a>
          <a href="#" target="_blank">[Code]</a>
          <a href="#" target="_blank">[Project Page]</a>
          <br>
          <abs>
            abstract, abstract, abstract, abstract, abstract, abstract, abstract, abstract, abstract, abstract, abstract, abstract, abstract, abstract, abstract, abstract, abstract, abstract, abstract, abstract, abstract, abstract, abstract, abstract
          </abs>
        </td>
      </tr>

      <!-- phyrecon -->
      <tr onmouseout="phyrecon_stop()" onmouseover="phyrecon_start()">
        <td style="width:30%;vertical-align:middle;padding-left:0px;padding-top:30px">
          <div class="one">
            <div class="two" id='phyrecon_image'>
              <img src='images/research/PhyRecon.gif' width="220"></div>
            <img src='images/research/PhyRecon.gif' width="220">
          </div>
          <script type="text/javascript">
            function phyrecon_start() {
              document.getElementById('phyrecon_image').style.opacity = "1";
            }

            function phyrecon_stop() {
              document.getElementById('phyrecon_image').style.opacity = "0";
            }
            phyrecon_stop()
          </script>
        </td>
        <td style="width:75%;vertical-align:middle;padding-left:5px;padding-top:0px">
          <papertitle>PhyRecon: Physically Plausible Neural Scene Reconstruction</papertitle>
          <br>
          <a href="https://dali-jack.github.io/Junfeng-Ni/ target="_blank"">Junfeng Ni*</a>,
          <a href="https://yixchen.github.io/" target="_blank">Yixin Chen*</a>,
          <a href="#" target="_blank">Bohan Jing</a>,
          <a href="https://pku.ai/author/nan-jiang/" target="_blank">Nan Jiang</a>,
          <a href="https://binwangbfa.github.io/" target="_blank">Bing Wang</a>,
          <a href="https://daibopku.github.io/daibo/" target="_blank">Bo Dai</a>,
          Puhao Li,
          <a href="https://yzhu.io/" target="_blank">Yixin Zhu</a>,
          <a href="https://zhusongchun.net/" target="_blank">Song-Chun Zhu</a>,
          <a href="https://siyuanhuang.com/" target="_blank">Siyuan Huang</a>
          <br>
          <em>arXiv</em> 2024
          <br>
          <a href="https://arxiv.org/abs/2404.16666" target="_blank">[Paper]</a>
          <a href="https://github.com/PhyRecon/PhyRecon" target="_blank">[Code]</a>
          <a href="https://phyrecon.github.io/" target="_blank">[Project Page]</a>
          <br>
          <abs>
            We introduce PhyRecon, which enables physically plausible 3D scene reconstruction. PhyRecon features a joint optimization framwork incorporating both differentiable rendering and physics-based objectives.
          </abs>
        </td>
      </tr>

      <!-- ag2manip -->
      <tr onmouseout="ag2manip_stop()" onmouseover="ag2manip_start()">
        <td style="width:30%;vertical-align:middle;padding-left:0px;padding-top:50px">
          <div class="one">
            <div class="two" id='ag2manip_image'>
              <img src='images/research/Ag2Manip.jpg' width="220"></div>
            <img src='images/research/Ag2Manip.jpg' width="220">
          </div>
          <script type="text/javascript">
            function ag2manip_start() {
              document.getElementById('ag2manip_image').style.opacity = "1";
            }

            function ag2manip_stop() {
              document.getElementById('ag2manip_image').style.opacity = "0";
            }
            ag2manip_stop()
          </script>
        </td>
        <td style="width:100%;vertical-align:middle;padding-left:5px;padding-top:0px">
          <papertitle>Ag2Manip: Learning Novel Manipulation Skills with Agent-Agnostic Visual and Action Representations</papertitle>
          <br>
          Puhao Li*,
          <a href="http://tengyu.ai/" target="_blank">Tengyu Liu*</a>,
          <a href="https://yuyangli.com" target="_blank">Yuyang Li</a>,
          <a href="https://sites.google.com/view/muzhihan/home" target="_blank">Muzhi Han</a>,
          <a href="https://geng-haoran.github.io/" target="_blank">Haoran Geng</a>,
          <a href="https://scholar.google.com/citations?user=uPk5l1EAAAAJ&hl=en" target="_blank">Shu Wang</a>,
          <a href="https://yzhu.io" target="_blank">Yixin Zhu</a>,
          <a href="https://www.zhusongchun.net/" target="_blank">Song-Chun Zhu</a>,
          <a href="https://siyuanhuang.com" target="_blank">Siyuan Huang</a>
          <br>
          <em>IROS</em> 2024 <font color='red'>(Oral Pitch)</font>
          <br>
          <a href="https://arxiv.org/abs/2404.17521" target="_blank">[Paper]</a>
          <!-- &nbsp -->
          <a href="https://github.com/xiaoyao-li/ag2manip" target="_blank">[Code]</a>
          <!-- &nbsp -->
          <a href="https://xiaoyao-li.github.io/research/ag2manip/" target="_blank">[Project Page]</a>
          <!-- &nbsp -->
          <!-- <a href="https://huggingface.co/spaces/SceneDiffuser/SceneDiffuserDemo" target="_blank">[Hugging Face]</a> -->
          <br>
          <abs>
            We introduce Ag2Manip, which enables various robotic manipulation tasks without any domain-specific demonstrations. Ag2Manip also supports robust imitation learning of manipulation skills in the real world.
          </abs>
        </td>
      </tr>
      
      <!-- multigrasp -->
      <tr onmouseout="multigrasp_stop()" onmouseover="multigrasp_start()">
        <td style="width:30%;vertical-align:middle;padding-top:40px;padding-bottom:0px;">
          <div class="one">
            <div class="two" id='multigrasp_image'>
              <img src='images/research/MultiGrasp.gif' width="220"></div>
            <img src='images/research/MultiGrasp.gif' width="220">
          </div>
          <script type="text/javascript">
            function multigrasp_start() {
              document.getElementById('multigrasp_image').style.opacity = "1";
            }

            function multigrasp_stop() {
              document.getElementById('multigrasp_image').style.opacity = "0";
            }
            multigrasp_stop()
          </script>
        </td>
        <td style="width:75%;vertical-align:middle;padding-left:5px;padding-top:0px;">
          <papertitle>Grasp Multiple Objects with One Hand</papertitle>
          <br>
          <a href="https://yuyangli.com" target="_blank">Yuyang Li</a>,
          <a href="https://benjamin-eecs.github.io/" target="_blank">Bo Liu</a>,
          <a href="https://gengyiran.github.io/" target="_blank">Yiran Geng</a>,
          Puhao Li,
          <a href="https://www.yangyaodong.com" target="_blank">Yaodong Yang</a>,
          <a href="https://yzhu.io" target="_blank">Yixin Zhu</a>,
          <a href="http://tengyu.ai/" target="_blank">Tengyu Liu</a>,
          <a href="https://siyuanhuang.com" target="_blank">Siyuan Huang</a>

          <br>
          <em>RA-L</em>, presented at <em>IROS</em> 2024 <font color='red'>(Oral Presentation)</font>
          <br>
          <a href="https://arxiv.org/abs/2310.15599" target="_blank">[Paper]</a>
          <a href="https://multigrasp.github.io/" target="_blank">[Code]</a>
          <a href="https://github.com/MultiGrasp/MultiGrasp/tree/graspem" target="_blank">[Data]</a>
          <a href="https://multigrasp.github.io/" target="_blank">[Project Page]</a>
          <br>
          <abs>
            We introduce MultiGrasp, a two-stage framework for simultaneous multi-object grasping with multi-finger dexterous hands. In addition, we contribute Grasp'Em, a large-scale synthetic multi-object grasping dataset.
          </abs>
        </td>
      </tr>

      <!-- affordmotion -->
      <tr onmouseout="affordmotion_stop()" onmouseover="affordmotion_start()">
        <td style="width:30%;vertical-align:middle;padding-left:0px;padding-top:45px">
          <div class="one">
            <div class="two" id='affordmotion_image'>
              <img src='images/research/AffordMotion.gif' width="220"></div>
            <img src='images/research/AffordMotion.gif' width="220">
          </div>
          <script type="text/javascript">
            function affordmotion_start() {
              document.getElementById('ag2manip_image').style.opacity = "1";
            }

            function affordmotion_stop() {
              document.getElementById('ag2manip_image').style.opacity = "0";
            }
            affordmotion_stop()
          </script>
        </td>
        <td style="width:100%;vertical-align:middle;padding-left:5px;padding-top:10px">
          <papertitle>Move as You Say, Interact as You Can: Language-guided Human Motion Generation with Scene Affordance</papertitle>
          <br>
          <a href="https://silvester.wang/" target="_blank">Zan Wang</a>,
          <a href="https://yixchen.github.io/" target="_blank">Yixin Chen</a>,
          <a href="https://buzz-beater.github.io/" target="_blank">Baoxiong Jia</a>,
          Puhao Li,
          <a href="https://jinluzhang.site/" target="_blank">Jinlu Zhang</a>,
          <a href="http://zhangjingze21.tech/" target="_blank">Jingze Zhang</a>,
          <a href="http://tengyu.ai/" target="_blank">Tengyu Liu</a>,
          <a href="https://yzhu.io" target="_blank">Yixin Zhu</a>,
          <a href="https://liangwei-bit.github.io/web/" target="_blank">Wei Liang</a>,
          <a href="https://siyuanhuang.com/" target="_blank">Siyuan Huang</a>
          <br>
          <em>CVPR</em> 2024 <font color='red'>(Highlight)</font>
          <br>
          <a href="https://arxiv.org/abs/2403.18036" target="_blank">[Paper]</a>
          <a href="https://github.com/afford-motion/afford-motion" target="_blank">[Code]</a>
          <a href="https://afford-motion.github.io/" target="_blank">[Project Page]</a>
          <br>
          <abs>
            We introduce a novel two-stage framework that employs scene affordance as an intermediate representation, effectively linking 3D scene grounding and conditional motion generation.
          </abs>
        </td>
      </tr>

      <!-- leo -->
      <tr onmouseout="leo_stop()" onmouseover="leo_start()">
        <td style="width:30%;vertical-align:middle;padding-left:0px;padding-top:25px">
          <div class="one">
            <div class="two" id='leo_image'>
              <img src='images/research/Leo-crop.jpg' width="220"></div>
            <img src='images/research/Leo-crop.jpg' width="220">
          </div>
          <script type="text/javascript">
            function leo_start() {
              document.getElementById('leo_image').style.opacity = "1";
            }

            function leo_stop() {
              document.getElementById('leo_image').style.opacity = "0";
            }
            leo_stop()
          </script>
        </td>
        <td style="width:75%;vertical-align:middle;padding-left:5px;padding-top:0px">
          <papertitle>An Embodied Generalist Agent in 3D World</papertitle>
          <br>
          <a href="https://huangjy-pku.github.io/" target="_blank">Jiangyong Huang*</a>,
          <a href="https://silongyong.github.io/" target="_blank">Silong Yong*</a>,
          <a href="https://jeasinema.github.io/" target="_blank">Xiaojian Ma*</a>,
          <a href="https://github.com/Germany321" target="_blank">Xiongkun Linghu*</a>,
          Puhao Li,
          <a href="https://github.com/jetpackfirstme" target="_blank">Yan Wang</a>,
          <a href="https://liqing-ustc.github.io/" target="_blank">Qing Li</a>,
          <a href="https://www.zhusongchun.net/" target="_blank">Song-Chun Zhu</a>,
          <a href="https://buzz-beater.github.io/" target="_blank">Baoxiong Jia</a>,
          <a href="https://siyuanhuang.com/" target="_blank">Siyuan Huang</a>
          <br>
          <em>ICML</em> 2024</em>
          <br>
          <em>ICLR</em> 2024 @ <em>LLMAgents Workshop</em>
          <br>
          <a href="https://arxiv.org/abs/2311.12871" target="_blank">[Paper]</a>
          <a href="https://github.com/embodied-generalist/embodied-generalist" target="_blank">[Code]</a>
          <a href="https://drive.google.com/drive/folders/1dko2dzdwRWSK3hi1liBpGHZ8Dz97jXdP" target="_blank">[Data]</a>
          <a href="https://embodied-generalist.github.io/" target="_blank">[Project Page]</a>
          <br>
          <abs>
            We introduce LEO, an embodied multi-modal and multi-task generalist agent that excels in perceiving, grounding, reasoning, planning, and acting in 3D world. 
            <!-- We also collect large-scale and comprehensive datasets for our generalist training scheme. -->
          </abs>
        </td>
      </tr>

      <!-- scenediffuser -->
      <tr onmouseout="scenediffuser_stop()" onmouseover="scenediffuser_start()">
        <td style="width:30%;vertical-align:middle;padding-left:0px;padding-top:40px;">
          <div class="one">
            <div class="two" id='scenediffuser_image'>
              <img src='images/research/SceneDiffuser-crop.png' width="220"></div>
            <img src='images/research/SceneDiffuser-crop.png' width="220">
          </div>
          <script type="text/javascript">
            function scenediffuser_start() {
              document.getElementById('scenediffuser_image').style.opacity = "1";
            }

            function scenediffuser_stop() {
              document.getElementById('scenediffuser_image').style.opacity = "0";
            }
            scenediffuser_stop()
          </script>
        </td>
        <td style="width:100%;vertical-align:middle;padding-left:5px;padding-top:10px;">
          <papertitle>Diffusion-based Generation, Optimization, and Planning in 3D Scenes</papertitle>
          <br>
          <a href="https://siyuanhuang.com/" target="_blank">Siyuan Huang*</a>,
          <a href="https://silvester.wang/" target="_blank">Zan Wang*</a>,
          Puhao Li,
          <a href="https://buzz-beater.github.io/" target="_blank">Baoxiong Jia</a>,
          <a href="http://tengyu.ai/" target="_blank">Tengyu Liu</a>,
          <a href="https://yzhu.io/" target="_blank">Yixin Zhu</a>,
          <a href="https://liangwei-bit.github.io/web/" target="_blank">Wei Liang</a>,
          <a href="https://www.zhusongchun.net/" target="_blank">Song-Chun Zhu</a>
          <br>
          <em>CVPR</em> 2023
          <br>
          <a href="https://arxiv.org/abs/2301.06015" target="_blank">[Paper]</a>
          <!-- &nbsp -->
          <a href="https://github.com/scenediffuser/Scene-Diffuser" target="_blank">[Code]</a>
          <!-- &nbsp -->
          <a href="https://scenediffuser.github.io/" target="_blank">[Project Page]</a>
          <!-- &nbsp -->
          <a href="https://huggingface.co/spaces/SceneDiffuser/SceneDiffuserDemo" target="_blank">[Hugging Face]</a>
          <br>
          <abs>
            We introduce SceneDiffuser, a unified conditional generative model for 3D scene understanding. In contrast to prior work, SceneDiffuser is intrinsically scene-aware, physics-based, and goal-oriented.
          </abs>
        </td>
      </tr>
      
      <!-- gendexgrasp -->
      <tr onmouseout="gendexgrasp_stop()" onmouseover="gendexgrasp_start()">
          <td style="width:30%;vertical-align:middle;padding-top:20px;padding-bottom:0px;">
            <div class="one">
              <div class="two" id='gendexgrasp_image'>
                <img src='images/research/GenDexGrasp.gif' width="220"></div>
              <img src='images/research/GenDexGrasp.gif' width="220">
            </div>
            <script type="text/javascript">
              function gendexgrasp_start() {
                document.getElementById('gendexgrasp_image').style.opacity = "1";
              }

              function gendexgrasp_stop() {
                document.getElementById('gendexgrasp_image').style.opacity = "0";
              }
              gendexgrasp_stop()
            </script>
          </td>
          <td style="width:75%;vertical-align:middle;padding-left:5px;padding-top:10px;">
            <papertitle>GenDexGrasp: Generalizable Dexterous Grasping</papertitle>
            <br>
            Puhao Li*,
            <a href="http://tengyu.ai/" target="_blank">Tengyu Liu*</a>,
            <a href="https://yuyangli.com" target="_blank">Yuyang Li</a>,
            <a href="https://gengyiran.github.io/" target="_blank">Yiran Geng</a>,
            <a href="https://yzhu.io" target="_blank">Yixin Zhu</a>,
            <a href="https://www.yangyaodong.com" target="_blank">Yaodong Yang</a>,
            <a href="https://siyuanhuang.com" target="_blank">Siyuan Huang</a>

            <br>
            <em>ICRA</em> 2023
            <br>
            <a href="https://arxiv.org/abs/2210.00722" target="_blank">[Paper]</a>
            <a href="https://github.com/tengyu-liu/GenDexGrasp/" target="_blank">[Code]</a>
            <a href="https://sites.google.com/view/gendexgrasp/multidex" target="_blank">[Data]</a>
            <a href="https://sites.google.com/view/gendexgrasp/home" target="_blank">[Project Page]</a>
            <br>
            <abs>
              We introduce GenDexGrasp, a versatile dexterous grasping method that can generalize to out-of-domain robotic hands.
              In addition, we contribute MultiDex, a large-scale synthetic dexterous grasping dataset.
            </abs>
          </td>
        </tr>

        <!-- dexgraspnet -->
        <tr onmouseout="dexgraspnet_stop()" onmouseover="dexgraspnet_start()">
        <td style="width:30%;vertical-align:middle;padding-top:35px;padding-bottom:0px;">
          <div class="one">
            <div class="two" id='dexgraspnet_image'>
              <img src='images/research/DexGraspNet-crop.jpg' width="220"></div>
            <img src='images/research/DexGraspNet-crop.jpg' width="220">
          </div>
          <script type="text/javascript">
            function dexgraspnet_start() {
              document.getElementById('dexgraspnet_image').style.opacity = "1";
            }

            function dexgraspnet_stop() {
              document.getElementById('dexgraspnet_image').style.opacity = "0";
            }
            dexgraspnet_stop()
          </script>
        </td>
        <td style="width:75%;vertical-align:middle;padding-left:5px;padding-top:30px;padding-bottom:0px;">
          <papertitle>DexGraspNet: A Large-Scale Robotic Dexterous Grasp Dataset for General Objects Based on Simulation</papertitle>
          <br>
          <a>Ruicheng Wang*</a>,
          <a>Jialiang Zhang*</a>,
          <a href="https://jychen18.github.io/" target="_blank">Jiayi Chen</a>,
          <a>Yinzhen Xu</a>,
          Puhao Li,
          <a href="http://tengyu.ai/" target="_blank">Tengyu Liu</a>,
          <a href="https://hughw19.github.io/" target="_blank">He Wang</a>
          
          <br>
          <em>ICRA</em> 2023 <font color='red'>(Oral Presentation, Outstanding Manipulation Paper Finalist)</font>
          <br>
          <a href="https://arxiv.org/abs/2210.02697" target="_blank">[Paper]</a>
          <a href="https://github.com/PKU-EPIC/DexGraspNet" target="_blank">[Code]</a>
          <a href="https://mirrors.pku.edu.cn/dl-release/DexGraspNet-ICRA2023/" target="_blank">[Data]</a>
          <a href="https://pku-epic.github.io/DexGraspNet/" target="_blank">[Project Page]</a>
          <br>
          <abs>
            We introduce a large-scale dexterous grasping dataset DexGraspNet, which based on simulation. 
            DexGraspNet features more physical stability and higher diversity than previous grasping datasets.
          </abs>
        </td>
      </tr>

    </tbody></table>
    
    <!-- EXPERIENCE -->
    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr>
        <td style="width:100%;vertical-align:middle;padding-top:60px;">
          <heading>Experience</heading>
        </td>
      </tr>
    </tbody></table>
    <table width="100%" align="center" border="0" cellpadding="20"><tbody>
      <tr>
        <td style="padding:10px;width:18px;vertical-align:middle;padding-top:20px"><img src="images/institution/tsinghua.svg", width="90ox"></td>
        <td style="padding-top:25px", width="100%" valign="center">
          <b>Tsinghua University</b>, China
          <br> 2023.09 - now
          <br>
          <br> <b>Ph.D. Student</b>
          <br> Advisor: Prof. <a href="https://www.zhusongchun.net/">Song-Chun Zhu</a>
        </td>
      </tr>
      <tr>
        <td style="padding:10px;width:18px;vertical-align:middle"><img src="images/institution/bigai.png", width="90px"></td>
        <td width="100%" valign="center">
          <b>Beijing Institute for General Artificial Intelligence (BIGAI)</b>, China
          <br> 2021.09 - now
          <br>
          <br> <b>Research Intern</b>
          <br> Advisor: Dr. <a href="http://tengyu.ai/">Tengyu Liu</a> and Dr. <a href="http://siyuanhuang.com/">Siyuan Huang</a> 
        </td>
      </tr>
      <tr>
        <td style="padding:10px;width:18px;vertical-align:middle"><img src="images/institution/tsinghua.svg", width="90ox"></td>
        <td width="100%" valign="center">
          <b>Tsinghua University</b>, China
          <br> 2019.08 - 2023.06
          <br>
          <br> <b>Undergraduate Student</b>
        </td>
      </tr>
    </tbody></table>


    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr>
        <td style="padding:0px">
          <br>
          <p style="text-align:right;font-size:small;">
            Fell free to contact me if you have any problem. Thanks for your visiting by &#128522
          <br>
          This page is designed based on <a href="https://jonbarron.info/" style="text-align:right;font-size:small;" target="_blank">Jon Barron</a>'s website and deployed on <a href="https://pages.github.com/" style="text-align:right;font-size:small;" target="_blank">Github Pages</a>.
          </p>
        </td>
      </tr>
    </tbody></table>
  </td>
  </tr>
  </table>
</body>

</html>
